Web Crawler

- fetch goroutine gets URL from buffer if: buffer is not empty and URL is in state "new"
- - attempt to retrieve robots.txt from root of site
- - - follow result handling based on https://developers.google.com/webmasters/control-crawl-index/docs/robots_txt
- - - - use https://github.com/temoto/robotstxt-go
- - If crawling allowed, perform GET on URL
- - parse all href attributes of <a> tags on page
- - - add link to buffer if:
- - - - link has a proper domain name (no IP addresses)
- - - - domain name ends with .com, .org, .net, .edu or .us (blacklist everything else for now)
- - goroutine finishes when page is indexed
- if buffer is empty, sleep for 15 seconds and make another attempt (up to max tries)

- Main allows an input for starting URL, number of concurrent goroutines, max tries
- results are written to standard output with the following JSON format:
- - {"url": "some href", "title": "some title", "parent": "base-36 encoded url of parent", "id": "base-36 encoded URL of item"}
- "id" is generated from the base-36 encoding of the URL
- - use strconv formatint?
- - "parent" is generated using the base-36 encoding of the URL of the parent page

Goroutines:
- goroutine to fetch and index urls
- goroutine to create fetch goroutines
- - will monitor the buffer and add new goroutines (up to max) as needed

Bonus:

Write output to a file
Write output to a database
Statistical Report on domain frequency
Support for configuration file for arguments